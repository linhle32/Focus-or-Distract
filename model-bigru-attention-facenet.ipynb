{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## biGRU-attention model for facenet-embedded-data\n",
    "\n",
    "Needs output data from <b>01-7 process-data-facenet</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, GRU, Attention, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import tensorflow.keras.backend as kr\n",
    "import pickle\n",
    "from data_split import train_valid_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#parameters\n",
    "test_rate = 0.15\n",
    "valid_rate = 0.15\n",
    "window = 30 #frame window\n",
    "seed = 123456\n",
    "\n",
    "####\n",
    "#load focus videos\n",
    "of = open('facenetdata.obj', 'rb')\n",
    "data, labels = pickle.load(of)\n",
    "of.close()\n",
    "\n",
    "####\n",
    "#transform classes\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "label_nums = OrdinalEncoder().fit_transform(labels)\n",
    "\n",
    "####\n",
    "#split into train/valid/test\n",
    "X_train,X_valid,X_test,y_train,y_valid,y_test = train_valid_test_split(data,label_nums,test_rate,valid_rate,window,seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#parameters\n",
    "gru_size = 512\n",
    "out_dense = [512]\n",
    "dropout = False\n",
    "droprate = 0.4\n",
    "\n",
    "####\n",
    "#functional codes\n",
    "face_in = tf.keras.Input(shape=(window,X_train_face.shape[-1]))\n",
    "# forward direction\n",
    "x = face_in\n",
    "x1 = GRU(gru_size, return_sequences=True)(x)\n",
    "x1 = BatchNormalization()(x1)\n",
    "# backward direction\n",
    "x2 = tf.reverse(x, axis=[1])\n",
    "x2 = GRU(gru_size, return_sequences=True)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "# combine\n",
    "x = tf.concat([x1,x2], axis=-1)\n",
    "if dropout:\n",
    "    x = Dropout(droprate)(x)\n",
    "# Self-Attention Block\n",
    "x = Attention()([x, x])\n",
    "x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "# Output Block\n",
    "for d in out_dense:\n",
    "    x = Dense(d, activation=\"relu\")(x)\n",
    "    if dropout:\n",
    "        x = Dropout(droprate)(x)\n",
    "output = Dense(units=1, activation='softmax')(x)\n",
    "# Build and Compile model\n",
    "model = Model(inputs=[face_in], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = SGD(lr=1e-4, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=[X_valid, y_valid], batch_size=512, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
